{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style> .rendered_html code { \n",
       "    padding: 2px 4px;\n",
       "    color: #c7254e;\n",
       "    background-color: #f9f2f4;\n",
       "    border-radius: 4px;\n",
       "} </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cmx\n",
    "%matplotlib inline\n",
    "\n",
    "# Some additional libraries which we'll use just\n",
    "# to produce some visualizations of our training\n",
    "from libs.utils import montage\n",
    "from libs import gif\n",
    "import IPython.display as ipyd\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# Bit of formatting because I don't like the default inline code style:\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"\"\"<style> .rendered_html code { \n",
    "    padding: 2px 4px;\n",
    "    color: #c7254e;\n",
    "    background-color: #f9f2f4;\n",
    "    border-radius: 4px;\n",
    "} </style>\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Autoencoders   -- unsurprised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- An autoencoder is a type of neural network that learns to encode its inputs, often using much less data. It does so in a way that it can still output the original input with just the encoded values. For it to learn, it does not require \"labels\" as its output. Instead, it tries to output whatever it was given as input. So in goes an image, and out should also go the same image. But it has to be able to retain all the details of the image, even after possibly reducing the information down to just a few numbers.\n",
    "- We'll also explore how this method can be extended and used to cluster or organize a dataset, or to explore latent dimensions of a dataset that explain some interesting ideas. For instance, we'll see how with handwritten numbers, we will be able to see how each number can be encoded in the autoencoder without ever telling it which number is which."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read mnist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting ../data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist= input_data.read_data_sets('../data/mnist/',one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:  [ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMfxEQjH8REL9HyG7ARQig1bsAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a310b70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## take a look at the first image\n",
    "i = mnist.train.images[0].reshape(28,28)\n",
    "plt.imshow(i,cmap='Greys')\n",
    "print('Label: ',mnist.train.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## take a look at firt 1000 images\n",
    "imgs = mnist.train.images[:400].reshape(-1,28,28)  ## unflaten 2d to 3d\n",
    "plt.imshow(montage(imgs),cmap='gray')              ## montage function is defined\n",
    "                                                   ## in utility "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take a look at mean and standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the mean across all images\n",
    "mean_img = np.mean(imgs, axis=0)\n",
    "plt.imshow(mean_img, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Take the mean across all images\n",
    "std_img = np.std(imgs, axis=0)\n",
    "plt.imshow(std_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now we can see, actually only the central part of the image chages, pixels on the edges does not really matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  We will started with fully connected model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To try and encode our dataset, we are going to build a series of fully connected layers that get progressively smaller. So in neural net speak, every pixel is going to become its own input neuron. And from the original 784 neurons, we're going to slowly reduce that information down to smaller and smaller numbers. It's often standard practice to use other powers of 2 or 10. I'll create a list of the number of dimensions we'll use for each new layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='pic/autoencoder_0.png',height=400, width = 900>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dimensions = [512, 256, 128, 64]             # reduce the feature thourgh each layer\n",
    "n_features = mnist.train.images.shape[1]     # training set is a matrix, each row is a picture, with 784 pixels \n",
    "X = tf.placeholder(tf.float32, [None, n_features])  # first dimention is none, to make it flexible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[784, 512]\n",
      "[512, 256]\n",
      "[256, 128]\n",
      "[128, 64]\n"
     ]
    }
   ],
   "source": [
    "##building the encoding network ###\n",
    "##################################\n",
    "current_input = X\n",
    "n_input = n_features\n",
    "Ws = []           # We're going to keep every matrix we create so let's create a list to hold them all\n",
    "\n",
    "# We'll create a for loop to create each layer, this is the same as we did in previous session:\n",
    "for layer_i, n_output in enumerate(dimensions):\n",
    "    with tf.variable_scope(\"encoder/layer/{}\".format(layer_i)): ## put each layer in scopes\n",
    "        W = tf.get_variable(\n",
    "            name='W',\n",
    "            shape=[n_input, n_output],\n",
    "            initializer=tf.random_normal_initializer(mean=0.0, stddev=0.02))\n",
    "        h = tf.matmul(current_input, W)\n",
    "        current_input = tf.nn.relu(h)\n",
    "        Ws.append(W)    # Finally we'll store the weight matrix so we can build the decoder.\n",
    "        n_input = n_output  ## change the dimension, make n_input to be n_output for next iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##building the decoding network ###\n",
    "##################################\n",
    "\n",
    "Ws = Ws[::-1]      # We'll first reverse the order of our weight matrices\n",
    "dimensions = dimensions[::-1][1:] + [ds.X.shape[1]] # then reverse the order of our dimensions\n",
    "print(dimensions)                                   # appending the last layers number of inputs.\n",
    "\n",
    "for layer_i, n_output in enumerate(dimensions):\n",
    "    with tf.variable_scope(\"decoder/layer/{}\".format(layer_i)):\n",
    "\n",
    "        # Now we'll grab the weight matrix we created before and transpose it\n",
    "        # So a 3072 x 784 matrix would become 784 x 3072\n",
    "        # or a 256 x 64 matrix, would become 64 x 256\n",
    "        W = tf.transpose(Ws[layer_i])\n",
    "\n",
    "        # Now we'll multiply our input by our transposed W matrix\n",
    "        h = tf.matmul(current_input, W)\n",
    "\n",
    "        # And then use a relu activation function on its output\n",
    "        current_input = tf.nn.relu(h)\n",
    "\n",
    "        # We'll also replace n_input with the current n_output, so that on the\n",
    "        # next iteration, our new number inputs will be correct.\n",
    "        n_input = n_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[64, 128, 256, 512]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
